{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "17PhBd_M5cxuSRzAa1qldVbUDLvOyfP1n",
      "authorship_tag": "ABX9TyPdy5LtksrdVEXYLFsmuDj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeommoKoo-dev/recommender-system/blob/beommo/book_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmndZ7QIuH6K"
      },
      "outputs": [],
      "source": [
        "!pip install keybert\n",
        "!pip install POT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keybert import KeyBERT\n",
        "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim import corpora\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "import spacy"
      ],
      "metadata": {
        "id": "p_qt7cKobRBP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "book_ids = []\n",
        "book_summaries = []\n",
        "synopsis = []\n",
        "bookCount = 200\n",
        "keywordCount = 15\n",
        "kw_model = KeyBERT()\n",
        "sp_model = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "with open('/content/sample_data/booksummaries.txt', 'r') as f:\n",
        "    bookReader = csv.reader(f, dialect='excel-tab')\n",
        "    count = 0\n",
        "    for bookData in bookReader:\n",
        "      if count >= bookCount :\n",
        "        break\n",
        "      if count % 10 == 0:\n",
        "        print(f'Reading File.. ({count}/{bookCount})')\n",
        "\n",
        "      data.append(bookData)\n",
        "      count += 1\n",
        "\n",
        "# convert data to pandas dataframe\n",
        "books = pd.DataFrame.from_records(data, columns=['book_id', 'freebase_id', 'book_title', 'author', 'publication_date', 'genre', 'summary'])"
      ],
      "metadata": {
        "id": "rYQAaL6E-N-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_ids = list(books['book_id'])\n",
        "bookSummary = list(books['summary'])"
      ],
      "metadata": {
        "id": "WsSD_3B3-xDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "for book_idx in range(0, bookCount):\n",
        "    if count >= bookCount:\n",
        "        break\n",
        "    if count % 10 == 0:\n",
        "        print(f'Reading File.. ({count}/{bookCount})')\n",
        "\n",
        "    keywords = kw_model.extract_keywords(bookSummary[book_idx], top_n=keywordCount, vectorizer=KeyphraseCountVectorizer())\n",
        "    keyList = [keyword[0] for keyword in keywords]\n",
        "    # extracting needless names\n",
        "    filtered_keywords = []\n",
        "    for keyword in keyList:\n",
        "        doc = sp_model(keyword)\n",
        "        is_name = any(token.ent_type_ == \"PERSON\" and token.ent_iob_ != '0' for token in doc)\n",
        "\n",
        "        if not is_name:\n",
        "            filtered_keywords.append(keyword)\n",
        "\n",
        "    keyString = ', '.join(filtered_keywords)\n",
        "\n",
        "    synopsis.append(keyString)\n",
        "\n",
        "    count += 1"
      ],
      "metadata": {
        "id": "tFUoaY2bSSgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing .csv\n",
        "csvFilePath = \"book_keyword_extraction_\" + str(keywordCount) + \".csv\"\n",
        "with open(csvFilePath, 'w', newline='', encoding='utf-8') as csvFile:\n",
        "    headers = ['BookId', \"Keywords\"]\n",
        "    writer = csv.DictWriter(csvFile, fieldnames=headers)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for idx in range(0, bookCount):\n",
        "        writer.writerow({'BookId': book_ids[idx], 'Keywords': synopsis[idx]})"
      ],
      "metadata": {
        "id": "JMxHn_DEwNHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "Jv0ndxdJl3z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "book_data = open(\"/content/sample_data/book_keyword_extraction_15.csv\", \"r\")\n",
        "movie_data = open(\"/content/sample_data/movie_keyword_extraction_15.csv\", \"r\")"
      ],
      "metadata": {
        "id": "YB7jCeLsmlMo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_rdr = csv.reader(book_data)\n",
        "movie_rdr = csv.reader(movie_data)"
      ],
      "metadata": {
        "id": "h5WfUuYrna7j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = {book[0]:book[1] for book in book_rdr}\n",
        "movies = {movie[0]:movie[1] for movie in movie_rdr}\n",
        "\n",
        "# for book in book_rdr :\n",
        "#   books.append(book)\n",
        "\n",
        "# for movie in movie_rdr :\n",
        "#   movies.append(movie)"
      ],
      "metadata": {
        "id": "yd8mkDHencvD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance = model.wmdistance(books['620'], movies['tt0105112'])"
      ],
      "metadata": {
        "id": "ap0Iledkns0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(movies)"
      ],
      "metadata": {
        "id": "FDnYQKi-K1k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_movie(book_id, recommend_count) :\n",
        "  if book_id not in books :\n",
        "    print(\"there is no book\")\n",
        "    return\n",
        "  book_keyword = books[book_id]\n",
        "  if len(book_keyword) == 0 :\n",
        "    print(\"there is no book summarization\")\n",
        "    return\n",
        "  count = 0\n",
        "  res = []\n",
        "\n",
        "  for movie_id, movie_keyword in movies.items() :\n",
        "    if len(movie_keyword) == 0 :\n",
        "      continue\n",
        "    distance = model.wmdistance(book_keyword, movie_keyword)\n",
        "    res.append({'movie_id' : movie_id, 'distance' : distance})\n",
        "\n",
        "  sorted_res = sorted(res, key=lambda x: x['distance'])\n",
        "  # print(sorted_res)\n",
        "  return sorted_res[:recommend_count]\n",
        ""
      ],
      "metadata": {
        "id": "V4ltNrbVJipW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = recommend_movie('620', 2)\n",
        "print(res)"
      ],
      "metadata": {
        "id": "roVyr_1fMn1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}