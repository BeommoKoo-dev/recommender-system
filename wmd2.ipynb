{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeommoKoo-dev/recommender-system/blob/beommo/wmd2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPwTHMYUbp1e"
      },
      "outputs": [],
      "source": [
        "# installs\n",
        "!pip install keybert\n",
        "!pip install POT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nXXQ4EcecLBd"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gensim.models import LdaModel\n",
        "from gensim import corpora\n",
        "from sklearn.manifold import TSNE\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZQbsLaSSjfL",
        "outputId": "44b27b5d-c2ae-49d3-a7ff-b4261d99719d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading File.. (0/200)\n",
            "Reading File.. (10/200)\n",
            "Reading File.. (20/200)\n",
            "Reading File.. (30/200)\n",
            "Reading File.. (40/200)\n",
            "Reading File.. (50/200)\n",
            "Reading File.. (60/200)\n",
            "Reading File.. (70/200)\n",
            "Reading File.. (80/200)\n",
            "Reading File.. (90/200)\n",
            "Reading File.. (100/200)\n",
            "Reading File.. (110/200)\n",
            "Reading File.. (120/200)\n",
            "Reading File.. (130/200)\n",
            "Reading File.. (140/200)\n",
            "Reading File.. (150/200)\n",
            "Reading File.. (160/200)\n",
            "Reading File.. (170/200)\n",
            "Reading File.. (180/200)\n",
            "Reading File.. (190/200)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "import spacy\n",
        "\n",
        "# How To Use\n",
        "# 1. Change user parameters\n",
        "# 2. Check jsonFilePath and csvFilePath in line 21, 44\n",
        "# 3. File will be made in current path\n",
        "\n",
        "# User Parameters\n",
        "movieCount = 200  # How many movies from .json\n",
        "keywordCount = 15 # How many keywords for each movie\n",
        "\n",
        "# Data Structures and Models\n",
        "movies = []\n",
        "synopsis = []\n",
        "rows = []\n",
        "kw_model = KeyBERT()\n",
        "sp_model = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Reading .json\n",
        "jsonFilePath = \"movie_data.json\"\n",
        "with open(jsonFilePath, 'r', encoding='utf-8') as jsonFile:\n",
        "    movieReader = (json.loads(line) for line in jsonFile)\n",
        "\n",
        "    count = 0\n",
        "    for movieData in movieReader:\n",
        "        if count >= movieCount:\n",
        "            break\n",
        "        if count % 10 == 0:\n",
        "            print(f'Reading File.. ({count}/{movieCount})')\n",
        "\n",
        "        movieID = movieData.get('movie_id', '')\n",
        "        movies.append(movieID)\n",
        "\n",
        "        movieSummary = movieData.get('plot_synopsis', '')\n",
        "        keywords = kw_model.extract_keywords(movieSummary, top_n=keywordCount)\n",
        "        keyList = [keyword[0] for keyword in keywords]\n",
        "        # extracting needless names\n",
        "        filtered_keywords = []\n",
        "        for keyword in keyList:\n",
        "            doc = sp_model(keyword)\n",
        "            is_name = any(token.ent_type_ == \"PERSON\" and token.ent_iob_ != '0' for token in doc)\n",
        "\n",
        "            if not is_name:\n",
        "                filtered_keywords.append(keyword)\n",
        "\n",
        "        keyString = ', '.join(filtered_keywords)\n",
        "\n",
        "        synopsis.append(keyString)\n",
        "\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0-9TCVkU1M1B"
      },
      "outputs": [],
      "source": [
        "# Writing .csv\n",
        "csvFilePath = \"movie_keyword_extraction_\" + str(keywordCount) + \".csv\"\n",
        "with open(csvFilePath, 'w', newline='', encoding='utf-8') as csvFile:\n",
        "    headers = ['MovieID', \"Keywords\"]\n",
        "    writer = csv.DictWriter(csvFile, fieldnames=headers)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for idx in range(0, movieCount):\n",
        "        writer.writerow({'MovieID': movies[idx], 'Keywords': synopsis[idx]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yBxTjHXIcMnP"
      },
      "outputs": [],
      "source": [
        "model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MRGSUibVOYRk"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "test_data = open(\"movie_keyword_extraction_15.csv\", \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "X5ob2wUCO8AR"
      },
      "outputs": [],
      "source": [
        "rdr = csv.reader(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPHlMaeOPWA7"
      },
      "outputs": [],
      "source": [
        "one = []\n",
        "two = []\n",
        "for row in rdr :\n",
        "  one.append(row[1])\n",
        "\n",
        "print(one[1])\n",
        "print(one[2])\n",
        "distance = model.wmdistance(one[1], one[5])\n",
        "print(distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beJlcuTqcYvb"
      },
      "outputs": [],
      "source": [
        "fin_one = open(\"/content/sample_data/test1.txt\", \"r\")\n",
        "fin_two = open(\"/content/sample_data/test2.txt\", \"r\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvkywsrRdjam"
      },
      "outputs": [],
      "source": [
        "summary_one = fin_one.readlines()\n",
        "summary_two = fin_two.readlines()\n",
        "\n",
        "summary_one = [w for w in summary_one if w not in stop_words]\n",
        "summary_two = [w for w in summary_two if w not in stop_words]\n",
        "\n",
        "\n",
        "wordCount = 20\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "keywords_first = kw_model.extract_keywords(summary_one, top_n=wordCount)\n",
        "keywords_second = kw_model.extract_keywords(summary_two, top_n=wordCount)\n",
        "\n",
        "\n",
        "keyList_first = []\n",
        "keyList_second = []\n",
        "for keyword in keywords_first:\n",
        "    keyList_first.append(keyword[0])\n",
        "for keyword in keywords_second:\n",
        "    keyList_second.append(keyword[0])\n",
        "far_distance_list = [\"hi\", \"hello\", \"unrelated\", \"list\", \"distance\", \"should\", \"be\", \"high\"]\n",
        "\n",
        "print(keyList_first)\n",
        "print(keyList_second)\n",
        "print(far_distance_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuFOtMUZdrft",
        "outputId": "8d3c0c17-d3a1-4030-b69e-03bb2012bfc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "distance = 1.2637\n",
            "distance = 1.3225\n"
          ]
        }
      ],
      "source": [
        "distance_dontknow = model.wmdistance(keyList_first, keyList_second)\n",
        "distance_high = model.wmdistance(keyList_first, far_distance_list)\n",
        "\n",
        "print('distance = %.4f' % distance_dontknow)\n",
        "print('distance = %.4f' % distance_high)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O33ke1ofG1I"
      },
      "outputs": [],
      "source": [
        "# ref : https://www.kaggle.com/code/yixuanzhou94/gensim-word-vector-visualization\n",
        "\n",
        "def display_pca_scatterplot(model,words=None,sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.vocab.keys()),sample)\n",
        "        else:\n",
        "            words = [word for word in model.vocab]\n",
        "\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        if word not in words :\n",
        "          continue\n",
        "        plt.text(x+0.05, y+0.05, word)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "1mTIlMpqfKak",
        "outputId": "a4c93237-35a2-46cb-e42d-9374375b5ee6"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-30d8f71aad79>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m display_pca_scatterplot(model,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         keyList_first)\n",
            "\u001b[0;32m<ipython-input-19-825d80ac4bd5>\u001b[0m in \u001b[0;36mdisplay_pca_scatterplot\u001b[0;34m(model, words, sample)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtwodim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-825d80ac4bd5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtwodim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'shawshank' not present\""
          ]
        }
      ],
      "source": [
        "display_pca_scatterplot(model,\n",
        "                        keyList_first)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1-2pOIMmSswEtqSAvmUsjhlV-Eyy1_O85",
      "authorship_tag": "ABX9TyPA+I2qzPOFG38z+qigoGuc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}